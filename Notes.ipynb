{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elements of Reinforcement Learning\n",
    "\n",
    "The main feature that distinguishes reinforcement learning from other types of learning is that it uses training information that *evaluates* the actions taken, rather than *instruct* by giving correct actions. Evaluative feedback depends entirely on the action taken, whereas instructive feedback used in supervised learning is independent of the action taken. Within reinforcement learning there is also a need for active exploration for an explicit search for good behavior - part of exploration vs exploitation dillema, where agent in encouraged to exploid its own past experience to pick actions that return the most rewards, but it also is encouraged to sometimes explore unseen states to see if they perhaps yield higher returns.\n",
    "\n",
    "Beyond the agent and the envrionment, there are 4 mains subelements of reinforcement learning system:\n",
    "\n",
    "- **policy** - defines the learning agent's way of behaving at a given time. It is a mapping from percived states of the environment to actions to be taken when in those states. It may be represented as a simple function, lookup table or extensive search process. It is the core of a reinforcement learning agent as it alone is sufficient to detmine behavior.\n",
    "\n",
    "- **reward** - defines the goal of a reinforcement learning problem. Each step, environment sends to the agent a single number called reward. Agent's role is to maximie the total reward it receives over the long run. The reward signal defines what are the good and bad events for the agent. The rewards can be thought of as immediate and defining features of the problem faced by the agents and it is the primary basis for altering the policy.\n",
    "\n",
    "- **value** function specified what is good in the long run. Value of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state. Whereas rewards determine the immediate, intrinsic desirability of environmental states, values indicate the long-term desirability of states after taking into account the states that are likely to follow and the rewards available in those states. The idea is that a state might give a low immediate reward, but still have a high value because it is regularly followed by other states that yield high rewards. Values are difficult to estimate, but play central role in learning optimal policy.\n",
    "\n",
    "- **model** of the environment, or something that mimics the behavior of the environment and allows inferences to be made about how the environment will behave. Models are used for planning, by which we mean any way deciding on a course of action by considering possible future situations before they are actually experiences. Methods for solving reinforcement learning problems that use models and planning are called *model-based* methods, as opposed to simpler *model-free* methoda that are explicitly trial-and-error learnings.\n",
    "\n",
    "### Notation\n",
    "\n",
    "- Estimated value of action ```a``` at time step ```t``` is denoted as $Q_t(a)$. We would like $Q_t(a)$ to be as close to $q_*(a)$, which represents optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
