{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming\n",
    "\n",
    "Dynamic Programming refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process. While classical DP is of limited use for reinforcement learning nowadays, because of their assumption of a perfect model and great computational expense, they are still the foundation for the understanding of more complex models and are important theoretically.\n",
    "\n",
    "The key idea of DP is the use of value functions to organize and structure the search for good policies. \n",
    "\n",
    "### Grid World\n",
    "\n",
    "We will use grid world toy example, where each agent can move up, down, left or right within a grid with previously specified rows and columns. Each move results in reward of -1. Top-left and bottom-right corners are terminal states are getting there gives agents the reward of 0. If agent makes a move that is out of bounds of the given grid, the agents stays in place instead and receives reward of -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GridWorld:\n",
    "   def __init__(self, rows=4, cols=4):\n",
    "      self.rows = rows\n",
    "      self.cols = cols\n",
    "      self.state = None\n",
    "      self.is_done = False\n",
    "      self.available_actions = ['up', 'down', 'left', 'right']\n",
    "      self.a2i = {'up': 0, 'down': 1, 'left': 2, 'right': 3}\n",
    "      self.i2a = {0: 'up', 1: 'down', 2: 'left', 3: 'right'}\n",
    "      self.reset()\n",
    "         \n",
    "   def reset(self):\n",
    "      self.grid = np.zeros((self.rows, self.cols))\n",
    "      for i in range(self.rows):\n",
    "         self.grid[i, :] = np.arange(self.cols)+self.cols*i\n",
    "      \n",
    "      self.state = np.random.randint(1, self.rows*self.cols)\n",
    "      self.n_states = self.rows*self.cols\n",
    "      self.is_done = False\n",
    "      \n",
    "   def step(self, action, state=None, look_ahead=False):\n",
    "      if self.is_done:\n",
    "         print('Agent reached terminal state in the previous move, please reset the environment!')\n",
    "         return\n",
    "      if state is None: state = self.state\n",
    "      \n",
    "      action = action.lower()\n",
    "      if action not in self.available_actions:\n",
    "         raise 'Invalid action'\n",
    "      \n",
    "      terminal = False\n",
    "      reward = -1\n",
    "      if action == 'up':\n",
    "         new_state = state - self.rows if state not in self.grid[0, :] else state\n",
    "      elif action == 'down':\n",
    "         new_state = state + self.rows if state not in self.grid[self.rows-1, :] else state\n",
    "      elif action == 'right':\n",
    "         new_state = state + 1 if state not in self.grid[:, self.cols-1] else state\n",
    "      elif action == 'left':\n",
    "         new_state = state - 1 if state not in self.grid[:, 0] else state\n",
    "         \n",
    "      if self.is_terminal(new_state):\n",
    "         terminal = True\n",
    "         reward = -1\n",
    "\n",
    "      if not look_ahead:\n",
    "         self.state = new_state\n",
    "         self.is_done = terminal\n",
    "         \n",
    "      return new_state, reward, terminal\n",
    "   \n",
    "   def is_terminal(self, state):\n",
    "      return state == 0 or state == (self.n_states-1)\n",
    "   \n",
    "   def __repr__(self):\n",
    "      vis = np.zeros_like(self.grid)\n",
    "      row = self.state // self.rows\n",
    "      col = self.state % self.cols\n",
    "      vis[row][col] = 1\n",
    "      return str(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0. 0. 0. 0.]\n",
       " [0. 1. 0. 0.]\n",
       " [0. 0. 0. 0.]\n",
       " [0. 0. 0. 0.]]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = GridWorld()\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, -1, False)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.step('down')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0. 0. 0. 0.]\n",
       " [0. 0. 0. 0.]\n",
       " [0. 1. 0. 0.]\n",
       " [0. 0. 0. 0.]]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Evaluation\n",
    "\n",
    "First, let's look into ways of computing the state-value function $v_\\pi$ for an arbitrary policy $\\pi$. We know that:\n",
    "\n",
    "$$v_\\pi(s)=E_\\pi [G_t|S_t=s]=\\sum_a \\pi(a|s)\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_\\pi (s')]$$\n",
    "\n",
    "where $\\pi(a|s)$ is the probability of taking action $a$ in state $s$ under policy $\\pi$. The expectations $E_\\pi$ are conditional on $\\pi$, thus the subscript.\n",
    "\n",
    "We can use *iterative policy evaluation* which applies the same operation to each state $s$: it replaces the old value of $s$ with new value obtained form the old values of the successor states of $s$, and the expected immediate rewards, along all the one-step transitions possible under the policy being evaluated. This is called *expected update* - each iteration of iterative policy evalution updates the value of every state once to produce the new approximate value function $v_{k+1}$. The updates are called *expected*, because they are based on expectation over all possible next states rather than on a sample next state. \n",
    "\n",
    "<img src='resources/iterative-policy-evaluation.JPG' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ipe(env, policy, theta=1e-5, gamma=1, n_steps=100, print_every=False):\n",
    "   '''\n",
    "   Iterative Policy Evaluation\n",
    "   Inputs:\n",
    "    - pi (dict): policy to be evaluated\n",
    "    - theta (float): threshold > 0 determining accuracy of estimation\n",
    "    - gamma (float): discount factor how to measure expected rewards received in far future\n",
    "    - n_steps (int): for how many steps to iterate through all states\n",
    "    - print_every (list): whether to print the curret value table estimation\n",
    "   '''\n",
    "   V = np.zeros(env.n_states)\n",
    "   i_step = 0.\n",
    "   while True and i_step < n_steps:\n",
    "      delta = 0.\n",
    "      V_tmp = V.copy()\n",
    "      \n",
    "      # Calculate\n",
    "      for s in range(1, env.n_states-1):\n",
    "         v = 0.\n",
    "         for a in env.available_actions:\n",
    "            new_s, r, done = env.step(a, state=s, look_ahead=True)\n",
    "            v += policy[s][a] * (r + gamma*V[new_s])\n",
    "         delta = max(delta, np.abs(v-V_tmp[s]))\n",
    "         V_tmp[s] = v\n",
    "         #if delta < theta: break\n",
    "      \n",
    "      # Print value estimation table\n",
    "      if print_every:\n",
    "         if i_step in print_every:\n",
    "            print(f'Iteration: {i_step}')\n",
    "            print(np.round(V.reshape(env.rows, env.cols), 1))\n",
    "            print('-'*50 + '\\n')\n",
    "            \n",
    "      V = V_tmp.copy()\n",
    "      i_step += 1\n",
    "      \n",
    "   return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = {key: {val: 0.25 for val in ['up', 'down', 'left', 'right']}for key in range(0, 16)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0.0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "--------------------------------------------------\n",
      "\n",
      "Iteration: 1.0\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "--------------------------------------------------\n",
      "\n",
      "Iteration: 2.0\n",
      "[[ 0.  -1.8 -2.  -2. ]\n",
      " [-1.8 -2.  -2.  -2. ]\n",
      " [-2.  -2.  -2.  -1.8]\n",
      " [-2.  -2.  -1.8  0. ]]\n",
      "--------------------------------------------------\n",
      "\n",
      "Iteration: 3.0\n",
      "[[ 0.  -2.4 -2.9 -3. ]\n",
      " [-2.4 -2.9 -3.  -2.9]\n",
      " [-2.9 -3.  -2.9 -2.4]\n",
      " [-3.  -2.9 -2.4  0. ]]\n",
      "--------------------------------------------------\n",
      "\n",
      "Iteration: 10.0\n",
      "[[ 0.  -6.1 -8.4 -9. ]\n",
      " [-6.1 -7.7 -8.4 -8.4]\n",
      " [-8.4 -8.4 -7.7 -6.1]\n",
      " [-9.  -8.4 -6.1  0. ]]\n",
      "--------------------------------------------------\n",
      "\n",
      "Iteration: 99.0\n",
      "[[  0.  -13.9 -19.9 -21.9]\n",
      " [-13.9 -17.9 -19.9 -19.9]\n",
      " [-19.9 -19.9 -17.9 -13.9]\n",
      " [-21.9 -19.9 -13.9   0. ]]\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.reset()\n",
    "V = ipe(g, random_policy, n_steps=100, print_every=[0, 1, 2, 3, 10, 99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Improvement\n",
    "\n",
    "One of the reasons for computing the value function is to find better policies. We know how good it is to follow the current policy from which the value function was estimated in the first place, but is it better or worse to change to the new policy? One way to answer that is to select $a$ in $s$ and follow the existing policy:\n",
    "\n",
    "$$q_\\pi(s,a)=\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_\\pi (s')]$$\n",
    "\n",
    "The criteria whether new action is better is dependent whether it is greater than or less than $v_\\pi(s)$. This check is called *policy improvement theorem*:\n",
    "\n",
    "$$q_\\pi(s,\\pi'(s)) \\geq v_\\pi(s)$$\n",
    "\n",
    "If the above equation is true, then $\\pi'$ must be as good as, or better than, $\\pi$ - it must obtain greater or equal expected return from all states $s \\in S$\n",
    "\n",
    "$$v_{\\pi'}(s) \\geq v_\\pi(s)$$\n",
    "\n",
    "Thus, knowing a policy and its value function, we can easily evaluate a change in he policy at a single state to a particular action. We can extend this to consider changes at *all* states and to *all* possible actions, selecting at each state the action that appears best according to $q_\\pi(s)$\n",
    "\n",
    "$$\\pi'(s)=argmax_a q_\\pi(s,a)=argmax_a \\sum_{s',r}p(s',r|s,a)[r+\\gamma v_\\pi(s')]$$\n",
    "\n",
    "This *greedy* policy takes action that looks best in short term - after one step of lookahead - according to $v$. By construction, if meets the conditions of the policy improvement theorem and thus must be as good (or better) than the original policy.\n",
    "\n",
    "We can iterate through policies by improving the first policy $\\pi$ using $v_\\pi$, which yields $\\pi'$. Then we can compute $v_{\\pi'}$ and improve it again to yield even better $\\pi''$ until we reach convergance at optimal $v_*$ and $\\pi_*$.\n",
    "\n",
    "<img src='./resources/policy-iteration.JPG' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipi(env, policy, V, gamma=1.):\n",
    "   '''Iterative Policy Improvement'''\n",
    "   policy_stable = True\n",
    "   for s in range(1, env.n_states-1):\n",
    "      old_action = np.argmax(list(policy[s].values()))\n",
    "      \n",
    "      action_eval = []\n",
    "      for a in env.available_actions:\n",
    "         new_s, r, done = env.step(a, state=s, look_ahead=True)\n",
    "         action_eval.append(r + gamma*V[new_s])\n",
    "      best_action = np.argmax(action_eval)\n",
    "      \n",
    "      if old_action != best_action:\n",
    "         policy_stable = False\n",
    "      \n",
    "      for a in env.available_actions:\n",
    "         if env.a2i[a] == best_action: \n",
    "            policy[s][a] = 1.\n",
    "         else: \n",
    "            policy[s][a] = 0.\n",
    "\n",
    "   return policy, policy_stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env):\n",
    "   # Start with random policy\n",
    "   policy = {key: {val: 1./len(env.available_actions) for val in env.available_actions} for key in range(0, env.n_states)}\n",
    "   done = False\n",
    "   \n",
    "   while not done:\n",
    "      # Policy Evaluation\n",
    "      V = ipe(env, policy, theta=1e-5, gamma=1, n_steps=100, print_every=False)\n",
    "\n",
    "      # Policy Improvement\n",
    "      policy, done = ipi(env, policy, V)\n",
    "\n",
    "   return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.reset()\n",
    "policy, V = policy_iteration(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " 1: {'up': 0.0, 'down': 0.0, 'left': 1.0, 'right': 0.0},\n",
       " 2: {'up': 0.0, 'down': 0.0, 'left': 1.0, 'right': 0.0},\n",
       " 3: {'up': 0.0, 'down': 1.0, 'left': 0.0, 'right': 0.0},\n",
       " 4: {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " 5: {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " 6: {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " 7: {'up': 0.0, 'down': 1.0, 'left': 0.0, 'right': 0.0},\n",
       " 8: {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " 9: {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " 10: {'up': 0.0, 'down': 1.0, 'left': 0.0, 'right': 0.0},\n",
       " 11: {'up': 0.0, 'down': 1.0, 'left': 0.0, 'right': 0.0},\n",
       " 12: {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " 13: {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 1.0},\n",
       " 14: {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 1.0},\n",
       " 15: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}}"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0., -1., -2., -3.],\n",
       "       [-1., -2., -3., -2.],\n",
       "       [-2., -3., -2., -1.],\n",
       "       [-3., -2., -1.,  0.]])"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.reshape(g.rows, g.cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['up' 'left' 'left' 'down']\n",
      " ['up' 'up' 'up' 'down']\n",
      " ['up' 'up' 'down' 'down']\n",
      " ['up' 'right' 'right' 'up']]\n"
     ]
    }
   ],
   "source": [
    "pick_best_action = lambda env, policy, state: env.i2a[np.argmax(list(policy[state].values()))]\n",
    "optimal_policy = np.array([pick_best_action(env, policy, s) for s in range(env.n_states)]).reshape(env.rows, env.cols)\n",
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
