{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming\n",
    "\n",
    "Dynamic Programming refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process. While classical DP is of limited use for reinforcement learning nowadays, because of their assumption of a perfect model and great computational expense, they are still the foundation for the understanding of more complex models and are important theoretically.\n",
    "\n",
    "The key idea of DP is the use of value functions to organize and structure the search for good policies. \n",
    "\n",
    "### Grid World\n",
    "\n",
    "We will use grid world toy example, where each agent can move up, down, left or right within a grid with previously specified rows and columns. Each move results in reward of -1. Top-left and bottom-right corners are terminal states are getting there gives agents the reward of 0. If agent makes a move that is out of bounds of the given grid, the agents stays in place instead and receives reward of -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "   def __init__(self, rows=4, cols=4):\n",
    "      self.rows = rows\n",
    "      self.cols = cols\n",
    "      self.state = None\n",
    "      self.is_done = False\n",
    "      self.available_actions = ['up', 'down', 'left', 'right']\n",
    "      self.reset()\n",
    "         \n",
    "   def reset(self):\n",
    "      self.grid = np.zeros((self.rows, self.cols))\n",
    "      for i in range(self.rows):\n",
    "         self.grid[i, :] = np.arange(self.cols)+self.cols*i\n",
    "      \n",
    "      self.state = np.random.randint(1, self.rows*self.cols)\n",
    "      self.n_states = self.rows*self.cols\n",
    "      self.is_done = False\n",
    "      \n",
    "   def step(self, action, state=None, look_ahead=False):\n",
    "      if self.is_done:\n",
    "         print('Agent reached terminal state in the previous move, please reset the environment!')\n",
    "         return\n",
    "      if state is None: state = self.state\n",
    "      \n",
    "      action = action.lower()\n",
    "      if action not in self.available_actions:\n",
    "         raise 'Invalid action'\n",
    "      \n",
    "      terminal = False\n",
    "      reward = -1\n",
    "      if action == 'up':\n",
    "         new_state = self.state - self.rows if self.state not in self.grid[0, :] else self.state\n",
    "      elif action == 'down':\n",
    "         new_state = self.state + self.rows if self.state not in self.grid[self.rows-1, :] else self.state\n",
    "      elif action == 'right':\n",
    "         new_state = self.state + 1 if self.state not in self.grid[:, self.cols-1] else self.state\n",
    "      elif action == 'left':\n",
    "         new_state = self.state - 1 if self.state not in self.grid[:, 0] else self.state\n",
    "         \n",
    "      if new_state == 0 or new_state == (self.n_states-1):\n",
    "         terminal = True\n",
    "         reward = 0\n",
    "\n",
    "      if not look_ahead:\n",
    "         self.state = new_state\n",
    "         self.is_done = terminal\n",
    "         \n",
    "      return new_state, reward, terminal\n",
    "   \n",
    "   def __repr__(self):\n",
    "      vis = np.zeros_like(self.grid)\n",
    "      row = self.state // self.rows\n",
    "      col = self.state % self.cols\n",
    "      vis[row][col] = 1\n",
    "      return str(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0. 0. 0. 0.]\n",
       " [0. 0. 0. 1.]\n",
       " [0. 0. 0. 0.]\n",
       " [0. 0. 0. 0.]]"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = GridWorld()\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, -1, False)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.step('up')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0. 0. 0. 1.]\n",
       " [0. 0. 0. 0.]\n",
       " [0. 0. 0. 0.]\n",
       " [0. 0. 0. 0.]]"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Evaluation\n",
    "\n",
    "First, let's look into ways of computing the state-value function $v_\\pi$ for an arbitrary policy $\\pi$. We know that:\n",
    "\n",
    "$$v_\\pi(s)=E_\\pi [G_t|S_t=s]=\\sum_a \\pi(a|s)\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_\\pi (s')]$$\n",
    "\n",
    "where $\\pi(a|s)$ is the probability of taking action $a$ in state $s$ under policy $\\pi$. The expectations $E_\\pi$ are conditional on $\\pi$, thus the subscript.\n",
    "\n",
    "We can use *iterative policy evaluation* which applies the same operation to each state $s$: it replaces the old value of $s$ with new value obtained form the old values of the successor states of $s$, and the expected immediate rewards, along all the one-step transitions possible under the policy being evaluated. This is called *expected update* - each iteration of iterative policy evalution updates the value of every state once to produce the new approximate value function $v_{k+1}$. The updates are called *expected*, because they are based on expectation over all possible next states rather than on a sample next state. \n",
    "\n",
    "![iterative policy evaluation](./resources/iterative-policy-evaluation.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ipe(env, policy, theta=1e-5, gamma=1, n_steps=100):\n",
    "   '''\n",
    "   Iterative Policy Evaluation\n",
    "   Inputs:\n",
    "    - pi: policy to be evaluated\n",
    "    - therta: threshold > 0 determining accuracy of estimation\n",
    "   '''\n",
    "   V = np.zeros(env.n_states)\n",
    "   i_step = 0.\n",
    "   while True and i_step < n_steps:\n",
    "      delta = 0.\n",
    "      for s in range(1, env.n_states-1):\n",
    "         v = 0.\n",
    "         for a in env.available_actions:\n",
    "            new_s, r, done = env.step(a, state=s, look_ahead=True)\n",
    "            v += policy[s][a] * (r + gamma*V[new_s])\n",
    "         delta = max(delta, abs(v-V[s]))\n",
    "         V[s] = v\n",
    "         if delta < theta: break\n",
    "      i_step += 1\n",
    "   return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = {key: {val: 0.25 for val in ['up', 'down', 'left', 'right']}for key in range(0, 16)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        , -72.1322314 , -72.1322314 , -72.49586777,\n",
       "       -73.2231405 , -73.2231405 , -73.2231405 , -73.2231405 ,\n",
       "       -73.58677686, -73.58677686, -73.58677686, -73.58677686,\n",
       "       -73.58677686, -73.58677686, -73.58677686,   0.        ])"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipe(g, random_policy, n_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " 1: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " 2: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " 3: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " 4: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " 5: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " 6: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " 7: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " 8: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " 9: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " 10: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " 11: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " 12: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " 13: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " 14: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " 15: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}}"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
